<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>STAT 360 Lecture 14</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/solarized.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
	<section>
	  The Poisson and Other Fish!
	  <br><img src="img/Poisson.jpg" width="300px"><br>
	  STAT 360 - Lecture 14
	</section>
	<section>
	  <h3>What we will cover today:</h3>
	  <ol>
	    <li class="fragment">Last time in Discretown:
	      <ul>
		<li>Negative Binomial</li>
		<li>Geometric</li>
		</ul>
	    <li class="fragment">Poisson Distribution</li>
	    </ol>
	</section>

	<section>
	  <section data-background-image="img/nb.png" data-background-opacity=".5"></section>
	  <section data-background-image="img/nb.png" data-background-opacity=".1">
	    <h3>Last Time in Discretown!</h3>
	    <h4>The Negative Binomial</h4>
	    <p>Some good questions were raised:</p>
	    <ul>
	      <li class="fragment">What's the deal with ${x - 1  \choose k -1}$?</li>
	      <li class="fragment">Why is it called negative?</li>
	      </ul>
	  </section>
	  <section data-background-image="img/nb.png" data-background-opacity=".5"></section>
	  <section data-background-image="img/nb.png" data-background-opacity=".1">
	    <h4>What's the deal with ${x - 1  \choose k -1}$?</h4>
	    <p class="fragment">Since the Negative Binomial distribution gives us the probability of having to run $x$ trials in order to get $k$ successes, then when we get the $k^{th}$ success we stop counting on the $x^{th}$ trial.</p>
	      <p class="fragment">This means that we can choose the prior $k-1$ successes from $x-1$ trials randomly and in whatever order they come.</p>
	  </section>
	   <section data-background-image="img/nb.png" data-background-opacity=".1">
	    <h4>Why is it called negative?</h4>
	    <p class="fragment">The formulation we learned from the book counts the number of trials needed to get $k$ successes.</p>
	    <p class="fragment">There are several different formulations for the same problem. Another is letting $X$ count the number of successes giver $r$ failures: ${k+r-1 \choose k}$.</p>
	    <p class="fragment">With a little bit of algebra: $\frac{(k+r-1)\dots(r)}{k!} = (-1)^k\frac{(-r)(-r-1)\dots(-r-k+1)}{k!} = (-1)^k{-r \choose k}$.</p>
	     <p class="fragment">So we are choosing successes from negative failures!</p>
	  </section>
	  <section>
	    <h4>Expectation and Variance</h4>
	    <p class="fragment">The expected value of a negative binomial r.v. is: $\mu = \frac{k}{p}$.</p>
	    <p class="fragment">The variance of a negative binomial r.v. is: $\sigma^2 = \frac{k(1-p)}{p^2}$.</p>
	    <p class="fragment">The expectation and variance of a geometric r.v. can be obtain from the above by letting $k=1$.</p>
	  </section>
	  <section data-background-image="img/baseball.jpg" data-background-opacity=".1">
	    <h4>Example</h4>
	    <p>Team $A$ plays team $B$ in a seven-game world series, so the series is over when either team wins four games.</p>
	    <p>For each game, $P(A$ wins$) = 0.6$, and the games are assumed independent.</p>
	    <p>What is the probability that the series will end in exactly six games?</p>
	  </section>
	  <section  data-background-image="img/dory.jpg" data-background-opacity=".1">
	    <h4>No Memory Property</h4>
	    <p>A special property of the geometric distribution is the no-memory property: If $X \sim Geo(p)$, then $P(X > j + k | X > j) = P(X > k)$.</p>
	    <p class="fragment">Given the CDF of the geometric distribution $G(x;p) = \sum_{i=1}^x p(1-p)^{i-1} = 1 - (1- p)^x$, for $x = 1,2,3,\dots$ (derived in a previous lecture), prove the no-memory property of the geometric distribution.</p>
	  </section>
	</section>
	<section>
	  <section>
	    <h3>The Poisson Distribution</h3>
	    <p>There are two main characteristics of a Poisson experiment:</p>
	    <div class="fragment">
	      <img src="img/FindingDory.jpg" width="300px">
	      <p>Wait a minute! Experiments with fish?!?</p>
	    </div>
	    <p class="fragment">No Dory...Poisson refers to the distribution...remember...ah...no, ok.</p>
	  </section>
	  <section  data-background-image="img/poispmf.png" data-background-opacity=".1">
	    <h3>The Poisson Distribution</h3>
	    <p>There are two main characteristics of a Poisson experiment:</p>
	    <ol>
	      <li class="fragment">We want to count the number of events occurring in a fixed interval of time or space if these events happen with a known average rate and independently of the time since the last event.</li>
	      <li class="fragment">We want to approximate a binomial experiment if the probability of success is "small" (such as 0.01) and the number of trials is "large" (such as 1,000).</li>
	      </ol>
	  </section>
	  <section data-background-image="img/mailboxes.jpg" data-background-opacity=".1">
	    <h4>For example:</h4>
	    <p>Suppose you typically get 4 pieces of mail per day. That becomes your expectation, but there will be a certain spread: sometimes a little more, sometimes a little less, once in a while nothing at all.</p>
	    <p class="fragment">Given only the average rate, for a certain period of observation (e.g. pieces of mail per day), and assuming that the process that produce the event flow is essentially random, the Poisson Distribution will tell you how likely it is that you will get 3, or 5, or 11, or any other number, during one period of observation.</p>
	  </section>
	  <section>
	    <p>That is, it predicts the degree of spread around a known average rate of occurrence.</p>
	  </section>
	  <section>
	    <h4>The Poisson p.m.f.</h4>
	    <p>Suppose we are watching for events and the number of observed events follows a Poisson distribution with rate $\lambda$. Then</p>
	    <p class="fragment">$$P(X=x) = \frac{\lambda^xe^{-\lambda}}{x!}$$</p>
	    <p class="fragment">where $x=0,1,2,\dots$</p>
	  </section>
	  <section data-background-image="img/horsekick.jpg" data-background-opacity=".2">
	    <h4>A Classic Example</h4>
	    <p>The classic Poisson example is the data set of von Bortkiewicz (1898), for the chance of a Prussian cavalryman being killed by the kick of a horse.</p>
	    <p class="fragment">Ten army corps were observed over 20 years, giving a total of 200 observations of one corps for a one year period. The total deaths from horse kicks were 122, and the average number of deaths per year per corps was thus 122/200 = 0.61.</p>
	  </section>
	  <section data-background-image="img/corphorse.jpg" data-background-opacity=".2">
	    <p>In any given year, we wouldn't expect to observe exactly 0.61 deaths in one corps (that is not possible; deaths occur in modules of 1), but sometimes none, sometimes one, occasionally two, perhaps once in a while three, and (we might intuitively expect) very rarely any more. </p>
	  </section>
	  <section>
	    <h4>A Classic Poisson Situation</h4>
	    A rare event, whose average rate is small, with observations made over many small intervals of time.For the entire set of Prussian data, where p = the predicted Poisson frequency for a given number of deaths per year, E is the corresponding number of years in which that number of deaths is expected to occur in our 200 samples (that is, our p value times 200), and A is the actual number of years in which that many deaths were observed, we have:
	    </section>
	    <section  data-background-image="img/horsekickbars.png" data-background-opacity=".1">
	   <table>
	     <thead>
	       <tr>
		 <th>Deaths</th>
		 <th>p</th>
		 <th>E</th>
		 <th>A</th>
	       </tr>
	     </thead>
	     <tbody>
	       <tr>
		 <td>0</td>
		 <td>0.54335</td>
		 <td>108.67</td>
		 <td>109</td>
	       </tr>
	       <tr>
		 <td>1</td>
		 <td>0.33145</td>
		 <td>66.29</td>
		 <td>65</td>
	       </tr>
	       <tr>
		 <td>2</td>
		 <td>0.10110</td>
		 <td>20.22</td>
		 <td>22</td>
	       </tr>
	       <tr>
		 <td>3</td>
		 <td>0.02055</td>
		 <td>4.11</td>
		 <td>3</td>
	       </tr>
	       <tr>
		 <td>4</td>
		 <td>0.00315</td>
		 <td>0.63</td>
		 <td>1</td>
	       </tr>
	       <tr>
		 <td>5</td>
		 <td>0.00040</td>
		 <td>0.08</td>
		 <td>0</td>
	       </tr>
	       <tr>
		 <td>6</td>
		 <td>0.00005</td>
		 <td>0.01</td>
		 <td>0</td>
	       </tr>
	     </tbody>
	   </table>
	    </section>
	</section>
	<section>
	  <section data-background-image="img/uber.jpg" data-background-opacity=".1">
	    <h4>Binomial in the Limit</h4>
	    <p>Uber is interested in solving the following problem: what is the probability of getting one ride request, two ride requests, etc from a particular location?</p>
	    <p class="fragment">After analyzing historical records, they conclude that the average request per minute is 5. Can they figure out their probability question just from this information?</p>
	  </section>
	 <section data-background-image="img/uber.jpg" data-background-opacity=".1">
	    <h4>Binomial in the Limit</h4>
	    <p>Let $X$ be the number of ride requests in the next minute. Let's find $P(X=k)$ using the binomial distribution!</p>
	    <p class="fragment">Split the minute into 60 seconds and count "success" for a request coming in a second and "failure" for no request coming in that second. If requests are equally likely, the the probability of success is 5/60.</p>
	  </section>
	  <section>
	    <img src="img/requests.png" width="1400px">
	    <p class="fragment">Great! so $X \sim Bin(n = 60, p = 5/60)$:</p>
	    <p class="fragment">$$P(X=k) = {60 \choose k}\left(\frac{5}{60}\right)^k\left(1-\frac{5}{60}\right)^{60-k}$$</p>
	    <p class="fragment">Uber great! But what if we get 2 request in a second??</p>
	  </section>
	  <section>
	    <small><p>Ok then, cut it in milliseconds!...and don't ask me what if 2 request in a millisecond!!</p>
	    <p class="fragment">Maybe Uber hires a mathematician to help:</p></small>
	    <img class="fragment" src="img/inthelimit.png" width="1400px">
	    </section>
	  <section>
	    <h4>Expectation of a Poisson R.V.</h4>
	    <img src="img/poissonex.png" width="700px">
	  </section>
	  <section>
	    <h4>Variance of a Poisson R.V.</h4>
	    <img src="img/poisvar.png" width="1400px">
	  </section>
	</section>
      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>
      // More info about config & dependencies:
      // - https://github.com/hakimel/reveal.js#configuration
      // - https://github.com/hakimel/reveal.js#dependencies
      Reveal.initialize({
      autoPlayMedia: true,
      math: {
      mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
      config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
      },
      dependencies: [
      { src: 'plugin/markdown/marked.js' },
      { src: 'plugin/markdown/markdown.js' },
      { src: 'plugin/notes/notes.js', async: true },
      { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
      { src: 'plugin/math/math.js', async: true }
      ]
      });
    </script>
  </body>
</html>
